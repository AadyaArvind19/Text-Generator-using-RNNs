# -*- coding: utf-8 -*-
"""IO FINAL PROJECT CODE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yIj0OjqfI5h_GXq7QP_SeA2djkmRDHvl
"""

!pip install tensorflow
!pip install keras.preprocessing

import numpy as np
import tensorflow as tf
import requests
from bs4 import BeautifulSoup
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Function to extract text from a webpage
def extract_text_from_webpage(url):
    # Send a GET request to the webpage
    response = requests.get(url)

    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract text from paragraph tags or any other relevant tags
        paragraphs = soup.find_all('p')  # Find all <p> tags (typically paragraphs)

        # Extract the text from these tags
        text = " ".join([para.get_text() for para in paragraphs])

        return text
    else:
        print(f"Failed to retrieve the webpage. Status code: {response.status_code}")
        return ""

# Function to preprocess the extracted text
def preprocess_text(text):
    # Split the text into sentences or lines
    texts = text.split('\n')  # You can also split by sentences or paragraphs

    # Clean the text (remove unnecessary whitespace)
    texts = [text.strip() for text in texts if text.strip()]

    return texts

# Function to create sequences from the text
def create_sequences(texts, tokenizer):
    # Create input sequences
    input_sequences = []
    for line in texts:
        token_list = tokenizer.texts_to_sequences([line])[0]
        for i in range(1, len(token_list)):
            n_gram_sequence = token_list[:i+1]
            input_sequences.append(n_gram_sequence)
    return input_sequences

# Function to prepare data for the model
def prepare_data(input_sequences, total_words):
    # Pad sequences
    max_sequence_len = max([len(seq) for seq in input_sequences])
    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

    # Create predictors and labels
    X, y = input_sequences[:, :-1], input_sequences[:, -1]
    y = tf.keras.utils.to_categorical(y, num_classes=total_words)

    return X, y, max_sequence_len

# Function to define and compile the RNN model
def create_model(total_words, max_sequence_len):
    model = Sequential([
        Embedding(total_words, 10, input_length=max_sequence_len-1),
        SimpleRNN(32),
        Dense(total_words, activation='softmax')
    ])

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Function to generate text from the trained model
def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predicted = model.predict(token_list, verbose=0)
        predicted = np.argmax(predicted, axis=-1)
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# Main execution
if __name__ == "__main__":
    # Specify the webpage URL you want to scrape
    url = 'https://en.wikipedia.org/wiki/Harry_Potter'  # Example: Wikipedia page of Harry Potter
    webpage_text = extract_text_from_webpage(url)

    # Preprocess the extracted text
    texts = preprocess_text(webpage_text)

    # Tokenize the text
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(texts)
    total_words = len(tokenizer.word_index) + 1  # +1 for padding token

    # Create input sequences from the tokenized text
    input_sequences = create_sequences(texts, tokenizer)

    # Prepare the data for training
    X, y, max_sequence_len = prepare_data(input_sequences, total_words)

    # Define and compile the model
    model = create_model(total_words, max_sequence_len)

    # Train the model
    model.fit(X, y, epochs=100, verbose=1)

import ipywidgets as widgets
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf

# Assuming the model, tokenizer, and max_sequence_len are already defined, like in the earlier code:

model = create_model(total_words, max_sequence_len)  # Model creation function
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)  # 'texts' is your preprocessed list
max_sequence_len = max(len(seq) for seq in input_sequences)

# Front-end Widgets

# Input fields
seed_text_widget = widgets.Text(
    value='Harry Potter',
    description='Seed Text:',
    disabled=False,
    style={'description_width': 'initial'}
)

num_words_widget = widgets.IntText(
    value=5,
    description='Num Words:',
    min=1,
    disabled=False,
    style={'description_width': 'initial'}
)

# Output display
output = widgets.Output()

# Define the action when the button is clicked
def on_button_click(b):
    with output:
        # Clear previous output
        output.clear_output()
        try:
            seed_text = seed_text_widget.value
            num_words = num_words_widget.value
            generated_text = generate_text(seed_text, num_words, model, tokenizer, max_sequence_len)
            print(f"Generated Text: {generated_text}")
        except Exception as e:
            print(f"Error generating text: {e}")

# Button to trigger text generation
generate_button = widgets.Button(description="Generate Text")
generate_button.on_click(on_button_click)

# Layout - arranging widgets vertically
input_widgets = widgets.VBox([seed_text_widget, num_words_widget])  # Stack the input widgets vertically
layout = widgets.VBox([input_widgets, generate_button, output])  # Stack input fields, button, and output vertically

# Display the widgets
display(layout)